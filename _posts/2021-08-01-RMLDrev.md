---
title: 'Riemannian manifold Langevin dynamics without the manifolds'
date: 2021-08-01
permalink: /posts/2021/08/RMLDrev/
tags:
  - Langevin dynamics
  - Markov chain Monte Carlo
  - Bayesian computation
---
I discuss how Riemannian manifold Langevin dynamics (RMLD) can be defined without appealing to manifolds by connecting RMLD with reversible perturbations. 


Quick review of Riemannian manifold Langevin dynamics
===

Let $\pi(x)$ be a target probability density on $\mathbb{R}^d$. Riemannian manifold Langevin dynamics (RMLD) has been presented as a better proposal for the Metropolis-adjusted Langevin algorithm (MALA) than standard Langevin dynamics (LD) \cite{girolami2011}. Let $X_t$ be a diffusion evolving according to the stochastic differential equation (SDE)
\begin{align}
   dX_t = \frac{1}{2}G^{-1}(X_t) \nabla \log \pi(X_t) \,dt + \frac{1}{2}\Omega(X_t) \, dt + \sqrt{G^{-1}(X_t)} \, dW_t
\end{align}
where $G(X_t)$ is the Riemmanian metric, which is a $d\times d$ positive-definite matrix, $W_t$ is a standard $d$-dimensional Brownian motion, and $\Omega(x)$ is a vector-valued function defined entry-wise as follows:
\begin{align} 
  \Omega_i(x) = |G(x)|^{-1/2} \sum_{j = 1}^d \frac{\partial}{\partial x_j}\left[(G^{-1})_{ij} (x) |G(x)|^{1/2}\right]. 
\end{align}
Here $|\cdot|$ denotes the determinant. In \cite{girolami2011}, the authors argue that using the Fisher information matrix will help with choosing $G(x)$. Some intuition for why using LD as a proposal for MCMC is attractive partially stems from the fact that LD has $\pi$ as its invariant density. It was noted in \cite{xifara2014}, however, that the invariant density of the RMLD presented above is not $\pi$! While the distribution it converges to is $\pi(\cdot)$, the density it samples from is defined with respect to the measure defined on the manifold! In fact, on $\mathbb{R}^d$, RMLD has invariant density $\pi^*(x) = \pi(x) |G(x)|^{1/2}$, and the note \cite{xifara2014} corrects RMLD so that it has $\pi(x)$ as its invariant density. We will walk through the steps for deriving the corrected version of RMLD in the sequel. 

Position-dependent MALA
===
The Fokker-Planck equation defines the evolution of densities of SDEs. Suppose $X_t$ is a diffusion process evolving in $\mathbb{R}^d$ according to
\begin{align}
  dX_t = a(X_t) \, dt + b(X_t) \, dW_t
\end{align}
and define $\eta(t,x)$ to be the density of $X_t$. The Fokker-Planck equation is 
\begin{align}
  \frac{\partial \eta(t,x)}{\partial t} = -\sum_{i} \frac{\partial}{\partial x_i} \left[a_i(x) \eta(t,x) \right] + \frac{1}{2} \sum_{i,j} \frac{\partial^2}{\partial x_i \partial x_j} \left[V_{ij}(x) \eta(t,x)\right] \label{eq:fp}
\end{align}
where $V(x) = b(x) b(x)^\*$. 
As a quick example, one can easily check that $\pi(x)$ is the invariant density of LD. Recall that LD is $dX_t = \nabla \log \pi(X_t)/2 \, dt + dW_t$. We have
\begin{align}
  -\sum_{i = 1}^d \frac{\partial}{\partial x_i}\left[ \frac{1}{2\pi} \frac{\partial \pi}{\partial x_i} \pi\right] + \frac{1}{2} \sum_{i = 1}^ d \frac{\partial^2 \pi}{\partial x_i^2} = - \frac{1}{2} \sum_{i = 1}^d \frac{\partial^2\pi}{\partial x_i^2} + \frac{1}{2} \sum_{i = 1}^ d \frac{\partial^2 \pi}{\partial x_i^2} = 0. 
\end{align}
Note, however, that this is not the only choice of SDE that has $\pi$ as the invariant density. Indeed, by appealing to \eqref{eq:fp}, if $a$ and $b$ are chosen such that
\begin{align\*}
  a_i(x) \pi(x) + \frac{1}{2} \sum_{j = 1}^d \frac{\partial}{\partial x_j}\left[ V_{ij}(x) \pi(x) \right] = 0, 
\end{align\*}
then the resulting diffusion process will have $\pi$ as the invariant density. The notion of preconditioning the gradient was the inspiration for pursuing RMLD \cite{girolami2011}, and they used Riemannian geometry concepts to derive the correction term $\Omega(x)$. In the same spirit, \cite{xifara2014} also supposed that we start with a diffusion of the form
\begin{align}
  dX_t = \frac{1}{2} A(X_t) \nabla \log \pi(X_t) \, dt+\frac{1}{2}\Gamma(X_t) dt + \sqrt{A(X_t)} \, dW_t \label{eq:pld}
\end{align}
where $A(x)$ is a positive-definite matrix and $\Gamma$ is the correction term. Using the conditions derived above, \cite{xifara2014} derives the correction term to be
\begin{align}
  \Gamma_i(x) = \sum_{j = 1}^d \frac{\partial A_{ij}(x)}{\partial x_j}. 
\end{align}
The resulting algorithm is called \emph{position-dependent MALA} or \emph{PMALA}. Clearly this is a much cleaner formulation than RM-MALA, and since it truly has $\pi(x)$ as the invariant density, numerical results show that PMALA performs better than RM-MALA on a few examples. More specifically, they show that the essential sample size (ESS) and ESS per second is larger for PMALA than for RM-MALA. See \cite{xifara2014} for further details. 




